{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "393d75e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\radhakrishna\\miniconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\radhakrishna\\miniconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: requests in c:\\users\\radhakrishna\\miniconda3\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\radhakrishna\\miniconda3\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\radhakrishna\\miniconda3\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\radhakrishna\\miniconda3\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\radhakrishna\\miniconda3\\lib\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\radhakrishna\\miniconda3\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\radhakrishna\\miniconda3\\lib\\site-packages (from requests) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\radhakrishna\\miniconda3\\lib\\site-packages (from requests) (2025.6.15)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn numpy requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b486e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Radhakrishna\\AppData\\Local\\Temp\\ipykernel_28664\\3587042943.py:16: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tarfile.open(path).extractall(\"data\")\n"
     ]
    }
   ],
   "source": [
    "import os, tarfile, urllib.request\n",
    "\n",
    "# Download and extract spam/ham datasets\n",
    "def get_data():\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    \n",
    "    urls = [\n",
    "        (\"https://spamassassin.apache.org/old/publiccorpus/20030228_spam.tar.bz2\", \"spam.tar.bz2\"),\n",
    "        (\"https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham.tar.bz2\", \"ham.tar.bz2\")\n",
    "    ]\n",
    "    \n",
    "    for url, file in urls:\n",
    "        path = f\"data/{file}\"\n",
    "        if not os.path.exists(path):\n",
    "            urllib.request.urlretrieve(url, path)\n",
    "        tarfile.open(path).extractall(\"data\")\n",
    "\n",
    "get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb318a2",
   "metadata": {},
   "source": [
    "# Email Spam Detection\n",
    "\n",
    "## Step 1: Getting Data \n",
    "\n",
    "- I downloaded **real email data** from the internet\n",
    "- This data has **examples** of both spam and good emails\n",
    "\n",
    "\n",
    "Downloaded: spam.tar.bz2 (500 spam emails) \n",
    "Downloaded: ham.tar.bz2 (2,500 good emails)\n",
    "Extracted to: data/ folder\n",
    "\n",
    "\n",
    "\n",
    "- **Spam emails**: So computer learns what bad emails look like\n",
    "- **Ham emails**: So computer learns what good emails look like\n",
    "  \n",
    "\n",
    "data/\n",
    "‚îú‚îÄ‚îÄ spam/           ‚Üê 500 bad emails \n",
    "‚îî‚îÄ‚îÄ easy_ham/       ‚Üê 2,500 good emails \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65c556f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 3002, Train: 2401, Test: 601\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get file paths and labels\n",
    "spam_files = [f\"data/spam/{f}\" for f in os.listdir(\"data/spam\")]\n",
    "ham_files = [f\"data/easy_ham/{f}\" for f in os.listdir(\"data/easy_ham\")]\n",
    "\n",
    "# Combine files and labels (1=spam, 0=ham)\n",
    "X = spam_files + ham_files\n",
    "y = [1] * len(spam_files) + [0] * len(ham_files)\n",
    "\n",
    "# Split data (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Total: {len(X)}, Train: {len(X_train)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2d610c",
   "metadata": {},
   "source": [
    "## Step 2: Teaching the Computer with Labels \n",
    "\n",
    "- This is called **\"Supervised Learning\"** because we supervise (teach) the computer\n",
    "\n",
    "**How I labeled my emails:**\n",
    "```python\n",
    "# Like flashcards for the computer:\n",
    "email1.txt ‚Üí SPAM (label = 1) \n",
    "email2.txt ‚Üí SPAM (label = 1)\n",
    "email3.txt ‚Üí GOOD (label = 0)\n",
    "email4.txt ‚Üí GOOD (label = 0)\n",
    "```\n",
    "\n",
    "**train/test split :**\n",
    "- **Training set (80%)**: Computer learns from these\n",
    "- **Test set (20%)**: I test if computer learned correctly\n",
    "\n",
    "\n",
    "**Why split the data?**\n",
    "- Computer might just memorize instead of actually learning\n",
    "\n",
    "**My dataset**: 3,002 total emails ‚Üí 2,401 for training, 601 for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fa36df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (2401, 52508)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Clean text function\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', 'URL', text)\n",
    "    text = re.sub(r'\\d+', 'NUMBER', text)\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "# Load and clean email\n",
    "def load_email(filename):\n",
    "    with open(filename, \"r\", encoding=\"latin-1\") as f:\n",
    "        content = f.read()\n",
    "        body = content[content.find('\\n\\n'):] if '\\n\\n' in content else content\n",
    "        return clean_text(body)\n",
    "\n",
    "# Process training data\n",
    "X_train_cleaned = [load_email(f) for f in X_train]\n",
    "\n",
    "# Create pipeline and transform data\n",
    "pipeline = Pipeline([('vectorizer', CountVectorizer(stop_words='english'))])\n",
    "X_train_transformed = pipeline.fit_transform(X_train_cleaned)\n",
    "\n",
    "print(f\"Shape: {X_train_transformed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b4f1a4",
   "metadata": {},
   "source": [
    "## Step 3: Cleaning the Text  \n",
    "\n",
    "\n",
    "\n",
    "### **BEFORE vs AFTER Example:**\n",
    "```\n",
    "Original email:\n",
    "\"VISIT WWW.SPAM.COM NOW!!! Call 123-456-7890 for $$$MONEY$$$\"\n",
    "\n",
    "Step 1: Make lowercase\n",
    "\"visit www.spam.com now!!! call 123-456-7890 for $$$money$$$\"\n",
    "\n",
    "Step 2: Replace websites with 'URL'\n",
    "\"visit URL now!!! call 123-456-7890 for $$$money$$$\"\n",
    "\n",
    "Step 3: Replace numbers with 'NUMBER' \n",
    "\"visit URL now!!! call NUMBER-NUMBER-NUMBER for $$$money$$$\"\n",
    "\n",
    "Step 4: Remove special characters (!@#$%^&*)\n",
    "\"visit URL now    call NUMBER NUMBER NUMBER for    money   \"\n",
    "\n",
    "Step 5: Fix extra spaces\n",
    "\"visit URL now call NUMBER NUMBER NUMBER for money\"\n",
    "```\n",
    "\n",
    " - Computer can see the pattern better for cleening the text.\n",
    "\n",
    "**Converting words to numbers (CountVectorizer):**\n",
    "```\n",
    "üìù Clean emails: [\"free money now\", \"get money fast\", \"hello friend\"]\n",
    "\n",
    "üî¢ Computer sees this table:\n",
    "           free  money  get  fast  hello  friend  now\n",
    "Email 1:    1     1     0    0     0      0      1\n",
    "Email 2:    0     1     1    1     0      0      0  \n",
    "Email 3:    0     0     0    0     1      1      0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efc694eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 95.67%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Ham       0.95      1.00      0.97       482\n",
      "        Spam       0.99      0.79      0.88       119\n",
      "\n",
      "    accuracy                           0.96       601\n",
      "   macro avg       0.97      0.89      0.93       601\n",
      "weighted avg       0.96      0.96      0.95       601\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Train model\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Process test data and predict\n",
    "X_test_cleaned = [load_email(f) for f in X_test]\n",
    "X_test_transformed = pipeline.transform(X_test_cleaned)\n",
    "y_pred = model.predict(X_test_transformed)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Ham', 'Spam']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2d1acb",
   "metadata": {},
   "source": [
    "## Naive Bayes! \n",
    "\n",
    "- It can read emails and decide if they're spam or not\n",
    "\n",
    "\n",
    " (Training phase) :\n",
    "```python\n",
    "model.fit(X_train_transformed, y_train)\n",
    "```\n",
    "1. I showed it had  2,401 emails with answers\n",
    "2. word patterns:\n",
    "   - \"Words like 'free', 'money', 'click' often appear in SPAM\"\n",
    "   - \"Words like 'meeting', 'project', 'thanks' often appear in GOOD emails\"\n",
    "\n",
    "\n",
    "Testing :\n",
    "```python\n",
    "y_pred = model.predict(X_test_transformed)\n",
    "```\n",
    "\n",
    "## RESULTS - \n",
    "\n",
    "**Overall Score: 95.67% Accuracy!** \n",
    "- AI got 575 out of 601 emails correct\n",
    "\n",
    "**Detailed Performance:**\n",
    "- **Ham (Good emails)**: 100% caught ‚úÖ (Never blocked important emails!)\n",
    "- **Spam (Bad emails)**: 79% caught ‚úÖ (21% spam got through)\n",
    "\n",
    "**In real life:**\n",
    "- Out of 100 emails: AI correctly sorts 96 emails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b52b0123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Logistic Regression Results ---\n",
      "Accuracy: 97.84%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Ham       0.98      1.00      0.99       482\n",
      "        Spam       0.98      0.91      0.94       119\n",
      "\n",
      "    accuracy                           0.98       601\n",
      "   macro avg       0.98      0.95      0.96       601\n",
      "weighted avg       0.98      0.98      0.98       601\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Train Logistic Regression model\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_lr = lr_model.predict(X_test_transformed)\n",
    "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
    "\n",
    "print(\"--- Logistic Regression Results ---\")\n",
    "print(f\"Accuracy: {accuracy_lr:.2%}\")\n",
    "print(classification_report(y_test, y_pred_lr, target_names=['Ham', 'Spam']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5028f0",
   "metadata": {},
   "source": [
    "## Comparing Different Models \n",
    "\n",
    "**Logistic Regression**\n",
    "\n",
    "\n",
    "**Naive Bayes ():**\n",
    "- This model acts like a probability calculator. Its main goal is to determine the probability that an email is spam, given the specific words inside it.\n",
    "- It studies the training emails to learn the probability of each word appearing in spam versus ham.\n",
    "- The \"Naive\" Assumption: The model's key feature (and weakness) is that it treats every word as independent. This means it assumes the presence of one word has no effect on another. It analyzes \"free\" and \"money\" separately, without understanding that their appearance together is extra suspicious. This simplification is why it's called \"naive.\n",
    "\n",
    "**Logistic Regression ():**\n",
    "- Instead of just counting, this model assigns a weight (or importance score) to every word in its vocabulary.\n",
    "- To classify a new email, it adds up the weights of all the words it contains. A high positive total score means \"spam,\" while a negative score means \"ham.\"\n",
    "- This allows it to learn context"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "display-name",
   "language": "python",
   "name": "yourenvname"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
